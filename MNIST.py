import random
import h5py
import numpy as np

import tensorflow as tf
import matplotlib.pyplot as plt

import Preproc
import Layers
import Nets


def load_HDF5():
    """Loads MNIST data set from .h5 file and returns it as a tuple of Numpy arrays.

    Returns
    -------
    dataTrain
        Numpy array with the training images.
    labelsTrain
        Numpy array with the training labels.
    dataTest
        Numpy array with the test imgaes.
    labelsTest
        Numpy array with the test labels.
    """
    with h5py.File('MNIST.h5', 'r') as f:
        dataTrain = np.expand_dims(np.array(f['Train']['images'])[:, :, :, 0], axis=-1)
        labelsTrain = np.array(f['Train']['labels']).reshape([-1])
        dataTest = np.expand_dims(np.array(f['Test']['images'])[:, :, :, 0], axis=-1)
        labelsTest = np.array(f['Test']['labels']).reshape([-1])

    return dataTrain, labelsTrain, dataTest, labelsTest


def generate_data_label_pair(data, labels):
    index = Preproc.generate_index(data.shape[0], shuffle=True)
    while True:
        indexAnchor = next(index)
        imageAnchor = data[indexAnchor]
        labelAnchor = labels[indexAnchor]

        image = [imageAnchor]
        label = [labelAnchor]

        yield image, label


def get_data_generators(batch_size, preproc_size=None):
    """ Creates generators that generate batches of samples from the MNIST data set.
    Parameters
    ----------
    batch_size : int
        Size of the batches generated by the generators returned by this method.
    preproc_size : list of ints
        Shape of each sample after pre-processing.

    Returns
    ----------
    Two generators: the first is for generating training data set batches, and the second is for generating test set
    batches.
    """
    if preproc_size is None:
        preproc_size = [28, 28, 1]
    (dataTrain, labelsTrain, dataTest, labelsTest) = load_HDF5()

    # preprocess training and test images by making sure they are 28x28x1
    def preprocess_images(images, size):
        results = np.ndarray([images.shape[0]] + size, np.uint8)
        for idx in range(images.shape[0]):
            distorted = images[idx]
            results[idx] = distorted.reshape([28, 28, 1])

        return results

    def generate_batch(training):
        if training:
            generator = generate_data_label_pair(dataTrain, labelsTrain)
        else:
            generator = generate_data_label_pair(dataTest, labelsTest)
        while True:
            batchImages = []
            batchLabels = []
            for _ in range(batch_size):
                images, labels = next(generator)
                batchImages.append(images)
                batchLabels.append(labels)
            batchImages = preprocess_images(np.concatenate(batchImages, axis=0), preproc_size)
            batchLabels = np.concatenate(batchLabels, axis=0)

            yield batchImages, batchLabels

    return generate_batch(training=True), generate_batch(training=False)


def get_adversarial_data_generators(batch_size, preproc_size=None):
    """ Creates generators that generate batches of samples from the MNIST data set, with pre-processed specifically for
    use when training the adversarial generator, and with a random adversarial target label.
    Parameters
    ----------
    batch_size : int
        Size of the batches generated by the generators returned by this method.
    preproc_size : list of ints
        Shape of each sample after pre-processing.

    Returns
    ----------
    Two generators: the first is for generating training data set batches, and the second is for generating test set
    batches. Each generator generates a tuple with three elements: a batch of images, a batch of their true labels, and
    a batch of the desired target labels.
    """
    if preproc_size is None:
        preproc_size = [28, 28, 1]
    (dataTrain, labelsTrain, dataTest, labelsTest) = load_HDF5()

    def preprocess_training_images(images, size):
        results = np.ndarray([images.shape[0]] + size, np.uint8)
        for idx in range(images.shape[0]):
            distorted = Preproc.randomFlipH(images[idx])
            distorted = Preproc.randomShift(distorted, rng=4)
            results[idx] = distorted.reshape([28, 28, 1])

        return results

    def preprocess_test_images(images, size):
        results = np.ndarray([images.shape[0]] + size, np.uint8)
        for idx in range(images.shape[0]):
            distorted = images[idx]
            distorted = Preproc.centerCrop(distorted, size)
            results[idx] = distorted

        return results

    def generate_batch(training):
        if training:
            generator = generate_data_label_pair(dataTrain, labelsTrain)
        else:
            generator = generate_data_label_pair(dataTest, labelsTest)

        while True:
            batchImages = []
            batchLabels = []
            batchTargets = []
            for _ in range(batch_size):
                images, labels = next(generator)
                batchImages.append(images)
                batchLabels.append(labels)
                batchTargets.append(random.randint(0, 9))

            if training:
                batchImages = preprocess_training_images(np.concatenate(batchImages, axis=0), preproc_size)
            else:
                batchImages = preprocess_test_images(np.concatenate(batchImages, axis=0), preproc_size)
            batchLabels = np.concatenate(batchLabels, axis=0)
            batchTargets = np.array(batchTargets)

            yield batchImages, batchLabels, batchTargets

    return generate_batch(training=True), generate_batch(training=False)


class NetMNIST(Nets.Net):
    # 10000-15000 steps is enough
    HParamMNIST = {'BatchSize': 200,
                   'LearningRate': 1e-3,
                   'MinLearningRate': 1e-5,
                   'DecayAfter': 3000,
                   'DecayRate': 0.30,
                   'ValidateAfter': 300,
                   'TestSteps': 50,
                   'TotalSteps': 40000}

    def __init__(self, image_shape, hyper_params=None):
        """
        Parameters
        ----------
        image_shape : list of ints
            Shape of images the net is trained on
        hyper_params : dictionary with string keys
            Dictionary mapping hyper param names to the the hyperparam values.

        Returns
        ----------
        Two generators: the first is for generating training data set batches, and the second is for generating test set
        batches. Each generator generates a tuple with three elements: a batch of images, a batch of their true labels, and
        a batch of the desired target labels.
        """
        Nets.Net.__init__(self)

        if hyper_params is None:
            hyper_params = NetMNIST.HParamMNIST

        self._init = False
        self._hyper_params = hyper_params

        with self._graph.as_default():
            # variable to keep check if network is being tested or trained
            self._ifTest = tf.Variable(False, name='ifTest', trainable=False, dtype=tf.bool)
            # define operations to set ifTest variable
            self._phaseTrain = tf.compat.v1.assign(self._ifTest, False)
            self._phaseTest = tf.compat.v1.assign(self._ifTest, True)

            self._step = tf.Variable(0, name='step', trainable=False, dtype=tf.int32)

            # Inputs
            self._images = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None] + image_shape,
                                          name='MNIST_images')
            self._labels = tf.compat.v1.placeholder(dtype=tf.int64, shape=[None],
                                          name='MNIST_labels_class')

            # define network body
            self._body = self.body(self._images)
            # define inference metric
            self._inference = self.inference(self._body)
            # defines accuracy metric. checks if inference output is equal to labels, and computes an average of the
            # number of times the output is correct
            self._accuracy = tf.reduce_mean(input_tensor=tf.cast(tf.equal(self._inference, self._labels), tf.float32))

            self._loss = self.loss(self._body, self._labels)
            # why reset the loss to 0 after defining it as a Tensor?
            self._loss = 0
            # add losses of all layers to NN loss tensor
            for elem in self._layers:
                if len(elem.losses) > 0:
                    for tmp in elem.losses:
                        self._loss += tmp

            # add update operations from all layers to the list of update ops of the Net.
            self._updateOps = []
            for elem in self._layers:
                if len(elem.update_ops) > 0:
                    for tmp in elem.update_ops:
                        self._updateOps.append(tmp)

            print(self.summary)
            print("\n Begin Training: \n")

            # Saver
            self._saver = tf.compat.v1.train.Saver(max_to_keep=5)

    def body(self, images):
        # normalise images to -1 to 1 values
        standardized = Preproc.normalise_images(images)
        # Body
        # paper says they used SmallNet in the experiment, not this VanillaNN
        net = Nets.vanilla_deep_fnn(standardized, self._layers)
        # net = Nets.SimpleV1C(standardized, self._step, self._ifTest, self._layers)

        # add output layer with no activation
        class10 = Layers.FullyConnected(net.output, outputSize=10, weightInit=Layers.XavierInit, wd=1e-4,
                                        biasInit=Layers.ConstInit(0.0),
                                        activation=Layers.Linear,
                                        name='FC_Coarse', dtype=tf.float32)
        self._layers.append(class10)

        return class10.output

    def inference(self, logits):
        return tf.argmax(input=logits, axis=-1, name='inference')

    def loss(self, logits, labels, name='cross_entropy'):
        net = Layers.CrossEntropy(logits, labels, name=name)
        self._layers.append(net)
        return net.output

    def train(self, training_data_generator, test_data_generator, path_load=None, path_save=None):
        with self._graph.as_default():
            # define decaying learning rate
            self._learning_rate = tf.compat.v1.train.exponential_decay(self._hyper_params['LearningRate'],
                                                             global_step=self._step,
                                                             decay_steps=self._hyper_params['DecayAfter'],
                                                             decay_rate=self._hyper_params['DecayRate'])
            self._learning_rate += self._hyper_params['MinLearningRate']

            # define optimiser
            self._optimizer = tf.compat.v1.train.AdamOptimizer(self._learning_rate, epsilon=1e-8).minimize(self._loss,
                                                                                                 global_step=self._step)
            # Initialize all variables
            self._sess.run(tf.compat.v1.global_variables_initializer())
            # check if it should re-start training from a known checkpoint
            if path_load is not None:
                self.load(path_load)

            self.evaluate(test_data_generator)

            # set testing flag to false
            self._sess.run([self._phaseTrain])
            if path_save is not None:
                self.save(path_save)

            # main training loop
            for _ in range(self._hyper_params['TotalSteps']):
                data, label = next(training_data_generator)
                # calculate loss and accuracy and perform one minimisation step
                loss, accuracy, step, _ = self._sess.run([self._loss, self._accuracy, self._step, self._optimizer],
                                                         feed_dict={self._images: data, self._labels: label})
                # what does this do exactly?
                self._sess.run(self._updateOps)

                # self.training_losses.append(loss)
                new_value = tf.stack([self.training_losses, tf.constant(loss)])
                self._sess.run([tf.compat.v1.assign(self.training_losses, new_value)])
                # self.training_accuracies.append(accu * 100)
                self._sess.run(
                    [tf.compat.v1.assign(self.training_accuracies, tf.stack([self.training_accuracies, tf.constant(accuracy)]))])

                # logging
                print('\rStep: ', step, '; loss: %.3f' % loss, '; accuracy: %.3f' % accuracy, end='')

                # evaluate on test set every so often
                if step % self._hyper_params['ValidateAfter'] == 0:
                    self.evaluate(test_data_generator)
                    # save state of model at each evaluation step
                    if path_save is not None:
                        self.save(path_save)

                    # set test flag back to False
                    self._sess.run([self._phaseTrain])

    def evaluate(self, test_data_generator, path=None):
        if path is not None:
            self.load(path)

        # set testing flag to true
        self._sess.run([self._phaseTest])

        sum_loss = 0.0
        sum_accuracy = 0.0
        # for each test step, measure loss and accuracy on test data, but do not train the weights!
        for _ in range(self._hyper_params['TestSteps']):
            data, label = next(test_data_generator)
            # evaluate loss and accuracy
            loss, accu = self._sess.run([self._loss, self._accuracy],
                                        feed_dict={self._images: data, self._labels: label})
            sum_loss += loss
            sum_accuracy += accu

        # calculate average loss and accuracy across the mini-batches in all test iterations
        avg_loss = sum_loss / self._hyper_params['TestSteps']
        avg_accuracy = sum_accuracy / self._hyper_params['TestSteps']
        self.test_losses.append(avg_loss)
        self.test_accuracies.append(avg_accuracy)

        print('\nTest: Loss: ', avg_loss,
              '; Accu: ', avg_accuracy)

    def infer(self, images):
        self._sess.run([self._phaseTest])

        return self._sess.run(self._inference, feed_dict={self._images: images})

    def save(self, path):
        self._saver.save(self._sess, path, global_step=self._step)

    def load(self, path):
        self._saver.restore(self._sess, path)


if __name__ == '__main__':
    net = NetMNIST([28, 28, 1])  # 8
    batchTrain, batchTest = get_data_generators(batch_size=NetMNIST.HParamMNIST['BatchSize'])
    net.train(batchTrain, batchTest, path_save='./ClassifyMNIST/netmnist.ckpt')

    plt.plot(net.training_losses.value(), label="Training set")
    plt.plot(net.test_losses, label="Test set")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("MNIST loss during training")
    plt.legend()
    plt.show()

    plt.plot(net.training_accuracies.value(), label="Training set")
    plt.plot(net.test_accuracies, label="Test set")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.title("MNIST accuracy during training")
    plt.legend()
    plt.show()
# The best configuration is 64 features and 8 middle layers
